# Celebrity Index Collector v1.0

A real-time celebrity sentiment analysis system for quantifying celebrity impact in Taiwan. Collects 24-hour search data on 100 Taiwan celebrities via Google Search API, processes text using Google Gemini AI for cleaning and sentiment analysis, stores results in PostgreSQL (local or Neon Cloud), and visualizes with an interactive Streamlit dashboard.

## ðŸš€ Quick Start

Get started in 30 seconds:

```bash
# 1. Clone and setup
git clone https://github.com/chatgpt003003-sudo/Vivi_v1.git
cd Vivi_v1
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cp .env.example .env  # Configure your API keys

# 2. Launch everything with one command
./vivi
```

That's it! The script will handle database setup, data collection, and launch the dashboard.

## Features

- **Real-time Data Collection**: Google Search API for last 24 hours of celebrity mentions (Traditional Chinese)
- **AI-Powered Processing**: Google Gemini 2.5 Flash for text cleaning and sentiment analysis
- **Sentiment Scoring**: -1.0 (negative) to +1.0 (positive) scale with thresholds (>0.3 positive, <-0.3 negative)
- **Interactive Dashboard**: Streamlit with 3 views - Rankings, Trends, and Statistics
- **100 Taiwan Celebrities**: Complete seed list with categories (singers, actors, athletes, politicians, etc)
- **Parallel Processing**: Concurrent celebrity processing with auto-worker detection
- **Cloud-Ready**: Local PostgreSQL or Neon PostgreSQL Cloud deployment
- **Auto-Schema**: Database table auto-creation on first run
- **Single-Command Launch**: `./vivi` handles setup, collection, and UI launch
- **Connection Pooling**: PostgreSQL connection management (1-20 connections)

## Tech Stack

**Backend**
- Python 3.13+
- PostgreSQL 14+ (Local or Neon Cloud)
- Google Custom Search API
- Google Gemini 2.5 Flash API

**Frontend**
- Streamlit 1.50.0 (port 8502)
- Plotly 6.3.1 (interactive charts)

**Libraries**
- `google-api-python-client` - Google Search integration
- `google-generativeai` - Gemini AI processing
- `psycopg2-binary` - PostgreSQL driver
- `pandas`, `numpy` - Data processing
- `concurrent.futures` - Parallel processing

## Project Structure

```
vivi/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection/      # Google Search API integration
â”‚   â”‚   â”œâ”€â”€ google_search.py
â”‚   â”‚   â””â”€â”€ celebrity_manager.py
â”‚   â”œâ”€â”€ data_processing/      # Text cleaning & sentiment analysis
â”‚   â”‚   â”œâ”€â”€ text_cleaner.py
â”‚   â”‚   â”œâ”€â”€ sentiment_analyzer.py
â”‚   â”‚   â””â”€â”€ pipeline.py
â”‚   â”œâ”€â”€ storage/              # Database layer
â”‚   â”‚   â”œâ”€â”€ db_connection.py
â”‚   â”‚   â””â”€â”€ schema.sql
â”‚   â””â”€â”€ ui/                   # Streamlit dashboard
â”‚       â””â”€â”€ app.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ api_config.py         # API configuration
â”‚   â””â”€â”€ celebrity_seed_list.json  # 100 celebrities
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_database.sh     # Database initialization
â”‚   â”œâ”€â”€ collect_sample_data.py # Sample data collector
â”‚   â””â”€â”€ run_ui.sh            # UI launcher
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                # Unit tests
â”‚   â””â”€â”€ integration/         # Integration tests
â”œâ”€â”€ vivi                     # Main CLI control script
â”œâ”€â”€ vivi.sh                  # Bash wrapper for CLI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example             # Environment template
â”œâ”€â”€ CLAUDE.md               # Claude Code instructions
â””â”€â”€ README.md               # This file
```

## Installation

### Prerequisites

- Python 3.13+
- PostgreSQL 14+
- Google Search API key
- Google Gemini API key

### Setup Steps

1. **Clone the repository**
```bash
git clone https://github.com/chatgpt003003-sudo/Vivi_v1.git
cd Vivi_v1
```

2. **Create virtual environment**
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure environment variables**
```bash
cp .env.example .env
# Edit .env with your API keys and database credentials
```

5. **Start PostgreSQL** (example for macOS with Homebrew)
```bash
brew services start postgresql@14
```

That's it! Now run:

```bash
./vivi
```

The script will automatically:
- Create the database and schema if needed
- Collect sample data from celebrities
- Launch the interactive dashboard

### Manual Database Setup

If you prefer to initialize the database manually:

```bash
psql -U your_username -c "CREATE DATABASE celebrity_index;"
psql -U your_username -d celebrity_index -f src/storage/schema.sql
```

## Usage

### Quick Start (One-Line Launch)

The simplest way to run the entire operation:

```bash
./vivi
```

Or if you prefer bash:

```bash
./vivi.sh
```

This will automatically:
1. âœ“ Check environment and dependencies
2. âœ“ Initialize database (if needed)
3. âœ“ Collect sample celebrity data
4. âœ“ Launch the interactive dashboard

Access the dashboard at: **http://localhost:8502**

### Individual Commands

You can also run specific operations:

```bash
# Environment check only
./vivi check

# Setup database only
./vivi setup

# Collect data only
./vivi collect

# Launch UI only (if data already exists)
./vivi ui
```

### Manual Launch (Traditional Method)

If you prefer to run components separately:

```bash
./scripts/run_ui.sh
# Or directly:
venv/bin/streamlit run src/ui/app.py
```

### Collect Celebrity Data

**Option 1: Sample data (10 celebrities)**
```bash
venv/bin/python scripts/collect_sample_data.py
```

**Option 2: Process specific celebrity**
```python
from src.data_processing.pipeline import DataPipeline

pipeline = DataPipeline()
result = pipeline.process_celebrity('å‘¨æ°å€«')
print(f'{result["name"]}: {result["sentiment"]:.2f}')
```

**Option 3: Batch process from seed list**
```python
from src.data_processing.pipeline import DataPipeline
from src.data_collection.celebrity_manager import CelebrityManager

pipeline = DataPipeline()
celebs = CelebrityManager.load_from_json('config/celebrity_seed_list.json')
summary = pipeline.process_multiple_celebrities(celebs, limit=20)
```

## Dashboard Features

Access at **http://localhost:8502**

### Rankings View
- Celebrity sentiment rankings (latest records with deduplication)
- Sortable: highest/lowest/most recent
- Top 20 bar chart (color-scaled by sentiment: red=negative, green=positive)
- Expandable celebrity details with cleaned text summary and source
- Metrics: Total celebrities, average sentiment, positive/neutral/negative counts

### Trend View
- Historical sentiment tracking over time
- Interactive line chart with markers
- Configurable date range selection (1-30 days)
- Threshold lines for positive/neutral/negative zones
- Historical data table with all records in date range

### Statistics View
- Total records and unique celebrities processed
- Average sentiment score
- Sentiment distribution donut chart
- Positive/neutral/negative breakdown counts
- Recent activity feed (latest 10 records)

## Configuration

### Environment Variables (.env)

**For Local Development:**
```bash
# Google APIs
GOOGLE_API_KEY=your_google_api_key
GOOGLE_SEARCH_ENGINE_ID=your_search_engine_id
GEMINI_API_KEY=your_gemini_api_key

# Local PostgreSQL Database
DB_NAME=celebrity_index
DB_USER=postgres
DB_PASSWORD=your_password
DB_HOST=localhost
DB_PORT=5432
```

**For Streamlit Cloud Deployment:**
- Create `secrets.toml` in `.streamlit/` with same keys
- Database: Neon PostgreSQL (auto-detected and used)
- Connection: Hardcoded Neon connection string for Cloud

### Celebrity Seed List

Edit `config/celebrity_seed_list.json` to add/remove celebrities:
```json
[
  {"name": "å‘¨æ°å€«", "category": "music"},
  {"name": "è”¡ä¾æž—", "category": "music"}
]
```

## Database Schema

```sql
CREATE TABLE celebrity_data (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    cleaned_paragraph TEXT,
    source TEXT,
    sentiment DECIMAL(5,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## Testing

Run unit tests:
```bash
pytest tests/unit/
```

Run integration tests:
```bash
pytest tests/integration/
```

## Development

### Code Structure

**Data Collection** (`src/data_collection/`)
- `google_search.py`: Google Custom Search API wrapper
  - Last 24 hours filtering (`dateRestrict='d1'`)
  - Traditional Chinese language (`lr='lang_zh-TW'`)
  - Rate limiting: 1 second between requests
  - Methods: `search_celebrity()`, `get_total_mentions()`
- `celebrity_manager.py`: 100-celebrity management
  - Load from `config/celebrity_seed_list.json`
  - Validate mention thresholds (100+ required)
  - Batch validation support

**Data Processing** (`src/data_processing/`)
- `pipeline.py`: Main orchestrator with **parallel processing**
  - `ThreadPoolExecutor` for concurrent processing (1-8 workers)
  - Auto-detect optimal worker count based on CPU/memory
  - Methods: `process_celebrity()`, `process_multiple_celebrities()`
  - Fallback to sequential if parallel fails
  - Returns summary with success rates
- `text_cleaner.py`: Google Gemini text summarization
  - Gemini 2.5 Flash model
  - Summarizes to 200 chars max
  - Extracts 3-5 key points
  - Graceful fallback on API failure
- `sentiment_analyzer.py`: Gemini sentiment scoring
  - Score range: -1.0 to +1.0
  - Thresholds: >0.3 positive, <-0.3 negative
  - Robust number extraction from API responses

**Storage** (`src/storage/`)
- `db_connection.py`: PostgreSQL connection management
  - Connection pooling (1-20 connections)
  - **Auto-schema initialization** on startup
  - Supports local and Neon Cloud databases
  - Methods: `get_connection()`, `return_connection()`, `initialize_schema()`

**UI** (`src/ui/`)
- `app.py`: Streamlit interactive dashboard
  - Three view modes: Rankings, Trends, Statistics
  - Real-time data queries with ROW_NUMBER deduplication
  - Plotly visualizations (bar, line, donut charts)
  - Database auto-initialization on first run

### Adding New Features

1. Create feature branch:
```bash
git checkout -b feature/your-feature-name
```

2. Make changes and test

3. Commit and push:
```bash
git add .
git commit -m "Add feature: description"
git push origin feature/your-feature-name
```

4. Create pull request on GitHub

## Troubleshooting

### Database Connection Issues
```bash
# Check PostgreSQL is running
ps aux | grep postgres

# Test connection
psql -U your_username -d celebrity_index -c "\dt"
```

### API Rate Limits
- Google Search API: 100 queries/day (free tier)
- Gemini API: 15 requests/minute
- Add delays between requests if hitting limits

### Import Errors
Ensure you're in the project root and virtual environment is activated:
```bash
cd /path/to/Vivi_v1
source venv/bin/activate
```

## Performance

- **Single celebrity processing**: ~5-10 seconds (search + clean + sentiment)
- **Batch (10 celebrities)**: ~30-60 seconds (sequential) or ~10-15 seconds (parallel)
- **Batch (100 celebrities)**: ~2-3 minutes (parallel with 4-8 workers)
- **Database queries**: <100ms with connection pooling and indexes
- **Dashboard load time**: 1-2 seconds

## API Rate Limits & Usage

**Google Search API**
- Free tier: 100 queries/day
- ~2 searches per celebrity
- Total: ~200 API calls for 100 celebrities daily

**Google Gemini 2.5 Flash**
- Rate limit: 15 requests/minute
- ~2 calls per celebrity (text cleaning + sentiment)
- Total: ~200 API calls for 100 celebrities daily

**PostgreSQL**
- Connection pooling: 1-20 active connections
- Dashboard queries: Optimized with indexes on `name`, `sentiment`, `created_at`

## Compliance & Ethics

- Respects API rate limits
- Collects only public information
- No personal data collection
- Source attribution maintained
- Blacklist support for restricted sources

## Status & Roadmap

### âœ… Completed (Test 1)
- Data collection from 100 Taiwan celebrities (Google Search)
- Google Gemini text cleaning and sentiment analysis
- PostgreSQL storage with connection pooling
- Interactive Streamlit dashboard with 3 views
- Parallel processing (1-8 worker threads)
- Local and Neon Cloud database support
- Auto-schema initialization
- Unit and integration tests
- Comprehensive CLI (`./vivi` one-liner)

### ðŸ”„ In Progress
- Streamlit Cloud deployment refinement
- Optional `psutil` dependency for resource detection
- Port 8502 configuration

### ðŸ“‹ Test 2 / Future Enhancements
- Daily automated data collection scheduling
- Additional data sources (social media APIs)
- Category-based sentiment aggregation
- Export functionality (CSV, JSON, PDF)
- Real-time alerts for sentiment changes >0.5 points
- Comparison tools between celebrities
- Advanced analytics and trend correlations
- Mobile-responsive UI
- REST API endpoints for external access

## Contributing

1. Fork the repository
2. Create feature branch
3. Make changes with tests
4. Submit pull request

## License

Copyright (c) 2025. All rights reserved.

## Acknowledgments

- Google Search API for data collection
- Google Gemini API for AI processing
- Streamlit for dashboard framework
- PostgreSQL for reliable data storage

## Contact

Repository: https://github.com/chatgpt003003-sudo/Vivi_v1

---

**Project Status**: âœ“ Test 1 Complete - Production Ready

Built with Claude Code - https://claude.com/claude-code
